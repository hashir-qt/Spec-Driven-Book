# Implementation Plan: Physical AI Book with RAG Chatbot

**Version**: 1.0.0  
**Status**: Active  
**Created**: 2024-12-06  
**Dependencies**: specification.md (SPEC-001)  

---

## Architecture Overview

### System Components

```
Frontend (Docusaurus)
├── Static Content (MDX chapters)
├── React Components (Chatbot, Auth, Personalization, Translation)
└── API Client (Axios with interceptors)
        │
        ├─→ FastAPI Backend
        │   ├── API Layer (Routes, validation)
        │   ├── Service Layer (Business logic)
        │   ├── Repository Layer (Data access)
        │   └── External Services (OpenAI Agents SDK, Qdrant for RAG)
        │
        ├─→ Neon Postgres (User data, preferences, chat history)
        ├─→ Qdrant Vector DB (RAG Embeddings)
        └─→ GEMINI API (flash-2.5) (Embeddings, chat completions)
```

### Technology Stack Decisions

**Decision 1: Monorepo vs Separate Repos**
- **Choice**: Monorepo with `/frontend` and `/backend` directories
- **Rationale**: Simplifies development, easier to maintain related code, single deployment pipeline
- **Tradeoff**: Slightly larger repo size, but manageable with .gitignore

**Decision 2: FastAPI vs Django**
- **Choice**: FastAPI
- **Rationale**: Async support (critical for OpenAI API calls), auto-generated OpenAPI docs, better type hints
- **Tradeoff**: Less built-in auth (mitigated by Better-auth), but more flexible

**Decision 3: Qdrant vs Pinecone vs Weaviate (for RAG)**
- **Choice**: Qdrant Cloud Free Tier
- **Rationale**: 1GB free storage, excellent Python SDK, open-source (can self-host if needed)
- **Tradeoff**: Smaller free tier than Pinecone (1GB vs 1M vectors), but sufficient for book content

**Decision 4: Better-auth vs NextAuth**
- **Choice**: Better-auth
- **Rationale**: Framework-agnostic, TypeScript-first, modern JWT approach
- **Tradeoff**: Newer library (less mature), but active development and good docs

---

## Implementation Phases

### Phase 1: Foundation Setup (Priority: P0)

**Goal**: Establish project structure, configure tools, validate tech stack integration.

**Deliverables**:
- Monorepo directory structure created
- Docusaurus initialized with sample chapter
- FastAPI backend with /health endpoint
- Neon Postgres connected (test query)
- Qdrant Cloud account created (test collection)
- OpenAI API key tested (embedding + chat)
- Development environment documented (README)

**Key Activities**:
1. Run `specifyplus init physical-ai-book` (if using Spec-Kit Plus CLI)
2. Create frontend/ with `npx create-docusaurus@latest`
3. Create backend/ with FastAPI boilerplate
4. Configure environment variables (.env.example)
5. Test all external API connections
6. Write setup documentation

**Success Criteria**:
- `npm start` runs Docusaurus locally
- `uvicorn app.main:app --reload` runs FastAPI locally
- All external services reachable (Neon, Qdrant, OpenAI)
- Documentation allows new developer to set up in < 30 minutes

**Estimated Time**: 4-6 hours

---

### Phase 2: Book Content Creation (Priority: P0)

**Goal**: Write educational chapters for 4 modules (ROS 2, Gazebo, Isaac, VLA).

**Deliverables**:
- Module 1 (ROS 2): 4 chapters with code examples
- Module 2 (Gazebo): 3 chapters with simulation tutorials
- Module 3 (NVIDIA Isaac): 3 chapters with perception examples
- Module 4 (VLA): 2 chapters with voice-to-action scenarios
- All chapters in MDX format (Docusaurus)
- Navigation configured (sidebars.ts)

**Key Activities**:
1. Research content using Context7 MCP (query official ROS 2, Gazebo, Isaac docs)
2. Write chapters following template:
   - Introduction (problem statement)
   - Concepts (theory with diagrams)
   - Code Examples (complete, runnable)
   - Exercises (practice problems)
3. Create Mermaid diagrams for architecture
4. Add screenshots/videos where helpful
5. Configure Docusaurus navigation

**Content Outline**:

**Module 1: ROS 2 Fundamentals (4 chapters)**
1. Introduction to ROS 2 (middleware, nodes, topics)
2. Building Your First Node (Python + rclpy)
3. Topics and Publishers/Subscribers (message passing)
4. Services and Actions (request/reply, long-running tasks)

**Module 2: Gazebo Simulation (3 chapters)**
1. Setting Up Gazebo Environment
2. URDF and Robot Modeling
3. Physics Simulation and Sensor Integration

**Module 3: NVIDIA Isaac (3 chapters)**
1. Isaac Sim Introduction (Omniverse, USD format)
2. Isaac ROS Perception Pipeline (VSLAM, object detection)
3. Sim-to-Real Transfer Techniques

**Module 4: Vision-Language-Action (2 chapters)**
1. Voice Commands with Whisper + LLMs
2. Cognitive Planning (Natural language → ROS 2 actions)

**Success Criteria**:
- 12 chapters minimum (target: 15-18)
- Each chapter 1000-3000 words
- All code examples tested and runnable
- Docusaurus navigation functional
- Mobile responsive

**Estimated Time**: 20-30 hours (can parallelize with AI assistance)

---

### Phase 3: RAG Chatbot Backend (Priority: P0)

**Goal**: Build RAG pipeline (embeddings → vector search → LLM response).

**Deliverables**:
- Embeddings script (reads MDX → generates vectors → uploads to Qdrant)
- Vector search service (queries Qdrant)
- Context builder (assembles LLM prompt)
- RAG engine (orchestrates full pipeline)
- API endpoint POST /api/v1/chat

**Key Activities**:
1. Write script `scripts/seed_vector_db.py`:
   - Parse MDX files (extract text, ignore frontmatter)
   - Chunk text (500-1000 tokens per chunk)
   - Generate embeddings (gemini-embedding-001)
   - Upload to Qdrant collection `book_chapters_en`
2. Implement vector search service:
   - Query Qdrant by embedding vector
   - Filter by relevance score > 0.6
   - Return top 5 results
3. Implement context builder:
   - Format search results for LLM
   - Add conversation history (last 3 messages)
   - Add selected text context (if provided)
   - Truncate to 6000 tokens max
4. Implement RAG engine:
   - Orchestrate: embed query → search → build context → call OpenAI Agents SDK for chat completion
   - Handle errors (API limits, no results found)
5. Create API endpoint:
   - Validate request (Pydantic schema)
   - Call RAG engine
   - Return response with sources

**Directory Structure**:
```
backend/app/
├── services/chatbot/
│   ├── embedder.py
│   ├── vector_search.py
│   ├── context_builder.py
│   └── rag_engine.py
├── api/v1/endpoints/
│   └── chatbot.py
└── schemas/
    └── chatbot.py
```

**Success Criteria**:
- Qdrant collection populated (verify vector count)
- POST /chat returns valid response
- Response time P95 < 3 seconds
- Answers cite source chapters
- Off-topic queries handled gracefully

**Estimated Time**: 8-12 hours

---

### Phase 4: Chatbot Frontend (Priority: P0)

**Goal**: Build React chatbot widget that calls backend API.

**Deliverables**:
- Floating chat widget (accessible on all pages)
- Chat interface (messages, input, send button)
- API integration (calls POST /chat)
- Error handling (loading states, error messages)
- Source display (clickable chapter links)

**Key Activities**:
1. Create ChatWidget component:
   - Floating button (bottom-right corner)
   - Expandable chat panel
   - Message list (user + assistant messages)
   - Input field + send button
2. Implement useChatbot hook:
   - API client (Axios with auth interceptors)
   - State management (messages, loading, error)
   - Error handling (retry logic, user-friendly messages)
3. Implement selected-text feature:
   - Detect text selection (window.getSelection())
   - Keyboard shortcut (Ctrl+Q or Cmd+Q)
   - Pass selected text as context to API
4. Style chat widget:
   - Mobile responsive (collapsible on small screens)
   - Accessibility (keyboard navigation, ARIA labels)
   - Match Docusaurus theme

**Directory Structure**:
```
frontend/src/
├── components/Chatbot/
│   ├── ChatWidget.tsx
│   ├── ChatMessage.tsx
│   ├── ChatInput.tsx
│   ├── SourceDisplay.tsx
│   └── ChatWidget.css
├── hooks/
│   └── useChatbot.ts
└── api/
    └── chatbot.ts
```

**Success Criteria**:
- Widget visible on all pages
- Messages display correctly
- API calls succeed (200 response)
- Loading indicator shows during processing
- Sources clickable (navigate to chapter)
- Works on mobile (375px width)

**Estimated Time**: 6-10 hours

---

### Phase 5: Authentication System (Priority: P1 - Bonus)

**Goal**: Implement signup/login with Better-auth, collect user background.

**Deliverables**:
- Better-auth configured (backend + frontend)
- Signup endpoint with background collection
- Login endpoint with JWT tokens
- Frontend auth UI (LoginModal, ProfileButton)
- Session management (token refresh)

**Key Activities**:
1. Install Better-auth:
   - Backend: `pip install better-auth-python` (or equivalent)
   - Frontend: `npm install better-auth`
2. Configure Better-auth backend:
   - Database schema (users table)
   - JWT settings (15-min access, 7-day refresh)
   - Password hashing (bcrypt)
3. Implement signup endpoint:
   - Validate email, password strength
   - Collect software_background, hardware_background (JSONB)
   - Return JWT tokens
4. Implement login endpoint:
   - Verify credentials
   - Return JWT tokens
   - Update last_login timestamp
5. Build frontend auth UI:
   - LoginModal (signup/login forms)
   - ProfileButton (user menu in navbar)
   - ProtectedRoute wrapper (for personalization features)
6. Implement token refresh:
   - Axios interceptor catches 401
   - Attempts token refresh
   - Retries original request

**Success Criteria**:
- User can signup with valid email + password
- Background fields collected (Python: Beginner/Intermediate/Advanced, etc.)
- User can login and receive JWT tokens
- Tokens stored securely (HttpOnly cookies or localStorage with CSRF protection)
- Auto-refresh on token expiry
- Logout clears tokens

**Estimated Time**: 8-12 hours

---

### Phase 6: Personalization Engine (Priority: P1 - Bonus)

**Goal**: Adapt chapter content based on user's background level.

**Deliverables**:
- Personalization API endpoint
- PersonalizeButton component (chapter-level)
- Content adaptation logic (calls OpenAI)
- Preference storage (user_preferences table)

**Key Activities**:
1. Design personalization prompt:
   - Input: Original chapter markdown, user background (JSON)
   - Output: Adapted markdown
   - Rules: Keep core facts, adjust examples/explanations
2. Implement personalization service:
   - Query user_preferences table
   - Call OpenAI Agents SDK with personalization prompt
   - Cache personalized content (avoid re-processing)
3. Create API endpoint POST /api/v1/personalize:
   - Requires authentication
   - Input: chapter_id, personalization_level
   - Output: Personalized markdown
4. Build PersonalizeButton component:
   - Visible only when authenticated
   - Dropdown: Beginner / Intermediate / Advanced
   - Calls API, replaces chapter content
   - "Reset to Original" button

**Personalization Prompt Example**:
```
Given this chapter content and user background, adapt the content:

User Background:
- Python: Advanced
- ROS 2: Beginner
- Linux: Intermediate

Chapter: "ROS 2 Nodes Introduction"

Rules:
1. Assume user knows Python well (skip basic syntax)
2. Explain ROS 2 concepts in detail (they're learning)
3. Use advanced Python examples (list comprehensions, decorators)
4. Keep technical accuracy (don't oversimplify)

Output: Adapted markdown
```

**Success Criteria**:
- "Personalize" button appears for logged-in users
- Clicking button adapts content
- Adapted content matches user's background level
- Original content restorable
- Preferences saved to database

**Estimated Time**: 6-10 hours

---

### Phase 7: Translation System (Priority: P1 - Bonus)

**Goal**: Translate chapters to Urdu on-demand.

**Deliverables**:
- Translation API endpoint
- TranslateButton component
- Urdu translation logic (OpenAI GPT-4)
- RTL layout support (CSS)

**Key Activities**:
1. Design translation prompt:
   - Input: English chapter markdown
   - Output: Urdu markdown
   - Rules: Keep technical terms in English, maintain formatting
2. Implement translation service:
   - Call OpenAI Agents SDK with translation prompt
   - Cache translations (avoid re-translating)
3. Create API endpoint POST /api/v1/translate:
   - Input: chapter_id, target_language ("ur")
   - Output: Translated markdown
4. Build TranslateButton component:
   - Toggle between English/Urdu
   - Apply RTL layout (CSS direction: rtl)
   - Code blocks remain LTR

**Translation Prompt Example**:
```
Translate this technical chapter to Urdu:

Chapter: "ROS 2 Nodes Introduction"

Rules:
1. Keep technical terms in English: "ROS 2 node", "publisher", "subscriber"
2. Translate explanations and descriptions
3. Maintain markdown formatting (headings, lists, code blocks)
4. Keep code blocks unchanged
5. Use professional, educational tone

Output: Urdu markdown
```

**Success Criteria**:
- "Translate to Urdu" button visible
- Clicking button translates chapter
- RTL layout applied correctly
- Technical terms remain in English
- Code blocks unchanged
- "Show Original" button restores English

**Estimated Time**: 6-8 hours

---

### Phase 8: Testing & Quality Assurance (Priority: P0)

**Goal**: Validate all features work, meet acceptance criteria.

**Deliverables**:
- Backend test suite (pytest)
- Frontend test suite (Jest + React Testing Library)
- Integration tests (E2E with Playwright)
- Performance tests (Lighthouse)
- Security audit

**Key Activities**:
1. Write backend unit tests:
   - RAG pipeline components
   - Auth service (signup, login, token refresh)
   - Personalization service
2. Write frontend component tests:
   - ChatWidget interactions
   - Auth forms validation
   - Personalization button behavior
3. Write integration tests:
   - User journey: Signup → Ask question → Personalize → Translate
   - API error scenarios
4. Run Lighthouse audit:
   - Performance > 85
   - Accessibility 100
   - Best Practices > 95
5. Security audit:
   - `npm audit` (fix critical/high vulnerabilities)
   - `safety check` (Python dependencies)
   - Manual review (SQL injection, XSS, CSRF)

**Success Criteria**:
- Test coverage > 70%
- All acceptance tests pass
- Lighthouse scores meet targets
- Zero critical security vulnerabilities
- Manual testing confirms all features work

**Estimated Time**: 8-12 hours

---

### Phase 9: Deployment (Priority: P0)

**Goal**: Deploy frontend and backend to production.

**Deliverables**:
- Frontend deployed to GitHub Pages or Vercel
- Backend deployed to Railway or similar
- Environment variables configured
- Monitoring enabled (Sentry)
- Documentation updated (deployment guide)

**Key Activities**:
1. Configure GitHub Actions (or Vercel):
   - Build Docusaurus
   - Deploy to GitHub Pages
2. Deploy backend to Railway:
   - Connect GitHub repo
   - Configure environment variables
   - Set up auto-deploy on push to main
3. Configure CORS:
   - Allow frontend domain only
4. Set up monitoring:
   - Sentry for error tracking (frontend + backend)
   - Basic uptime monitoring
5. Test production deployment:
   - Verify all endpoints accessible
   - Test chatbot end-to-end
   - Check auth flow works

**Success Criteria**:
- Book accessible at public URL
- Chatbot functional in production
- No CORS errors
- Environment variables secure (not exposed)
- Monitoring configured

**Estimated Time**: 4-6 hours

---

### Phase 10: Reusable Intelligence (Priority: P1 - Bonus)

**Goal**: Create custom Agent Skills and Subagents for Gemini CLI.

**Deliverables**:
- 2+ custom skills (e.g., RAG implementation, Auth integration)
- 1+ subagent (e.g., Content writer, API tester)
- Documented in history/prompts/ as PHRs

**Key Activities**:
1. Identify recurring patterns during development:
   - Example: "Write RAG pipeline code following specific architecture"
   - Example: "Implement API endpoint with validation, error handling, tests"
2. Design custom skills:
   - Define persona, questions, principles
   - Test with Gemini CLI
3. Design subagent:
   - Example: "Content Writer" subagent for book chapters
   - Specialization: Technical writing, code examples, pedagogical structure
4. Document in PHRs:
   - What prompts worked well
   - What prompts failed
   - How to invoke skills/subagents

**Success Criteria**:
- At least 2 skills created and tested
- At least 1 subagent created and tested
- PHRs document what worked vs failed
- Skills actively used during final implementation phase

**Estimated Time**: 4-6 hours

---

## Dependency Graph

```
Phase 1 (Foundation)
    ├──→ Phase 2 (Book Content) ──→ Phase 3 (RAG Backend)
    │                                       ↓
    └──→ Phase 4 (Chatbot Frontend) ←──────┘
                ↓
    Phase 5 (Authentication)
                ↓
    ├──→ Phase 6 (Personalization)
    └──→ Phase 7 (Translation)
                ↓
    Phase 8 (Testing)
                ↓
    Phase 9 (Deployment)
    
    Phase 10 (Reusable Intelligence) → Parallel to all phases
```

**Critical Path**: Phase 1 → 2 → 3 → 4 → 8 → 9 (Base deliverable)

**Parallel Work**: Phases 6, 7, 10 can overlap with earlier phases.

---

## Resource Requirements

### Development Environment
- **Hardware**: Laptop with 16GB+ RAM (for running Docusaurus + FastAPI locally)
- **Software**: 
  - Node.js 18+
  - Python 3.12+
  - Git
  - Code editor (VS Code recommended)
  - Gemini CLI configured with MCP servers (Context7, GitHub)

### External Accounts Needed
- OpenAI API key (pay-as-you-go, ~$20-50 budget)
- Qdrant Cloud account (free tier)
- Neon Serverless Postgres (free tier)
- GitHub account (for deployment + version control)
- Vercel or Railway account (for backend hosting)

### Time Estimate
- **Base deliverable (100 points)**: 60-80 hours
- **All bonus features (250 points)**: 90-120 hours
- **Includes**: Learning, debugging, documentation, testing

---

## Risk Mitigation

### Risk 1: OpenAI API Rate Limits
- **Mitigation**: Upgrade to Tier 1 ($5 minimum), implement exponential backoff, cache embeddings

### Risk 2: Qdrant Storage Exceeds 1GB
- **Mitigation**: Chunk chapters to 500 tokens (smaller chunks), monitor usage, compress if needed

### Risk 3: Authentication Integration Complexity
- **Mitigation**: Use Context7 MCP to query Better-auth docs, allocate extra time (8-12h instead of 6h)

### Risk 4: Translation Quality Issues
- **Mitigation**: Iterate on translation prompt, test with sample chapters, have native speaker review

### Risk 5: Deadline Pressure
- **Mitigation**: Focus on base deliverable first (100 points), add bonus features incrementally

---

## Success Metrics

**Phase Completion**:
- Phase 1-4, 8-9: Must complete (base deliverable)
- Phase 5-7, 10: Optional (bonus points)

**Quality Gates**:
- All acceptance tests pass
- Lighthouse performance > 85
- Test coverage > 70%
- Zero critical security vulnerabilities

**Submission Checklist**:
- [ ] Book deployed and accessible
- [ ] Demo video recorded (< 90 seconds)
- [ ] GitHub repo public
- [ ] README complete
- [ ] Form submitted

---

## Revision History

| Version | Date       | Changes                          | Author |
|---------|------------|----------------------------------|--------|
| 1.0.0   | 2024-12-06 | Initial implementation plan      | AI     |


# Implementation Plan: Physical AI Book with Multi-Agent RAG Chatbot

**Version**: 2.0.0  
**Status**: Active  
**Created**: 2024-12-06  
**Updated**: 2024-12-06 (OpenAI Agents SDK Architecture)  
**Dependencies**: specification.md (SPEC-002)  

---

## Architecture Overview (Updated)

### Multi-Agent System Components

```
Frontend (Docusaurus)
├── Static Content (MDX chapters)
├── React Components (Chatbot, Auth, Personalization, Translation)
└── API Client (Axios with interceptors)
        │
        ├─→ FastAPI Backend
        │   ├── Routes (API endpoints)
        │   ├── Agents (OpenAI Agents SDK)
        │   │   ├── Orchestrator (query routing)
        │   │   ├── Concept Explainer Agent
        │   │   ├── Code Helper Agent
        │   │   └── Troubleshooting Agent
        │   ├── Chatbot (RAG services)
        │   │   ├── RAG Service (orchestration)
        │   │   ├── Vector Search (Qdrant)
        │   │   ├── Embedder (Gemini)
        │   │   └── Context Builder
        │   ├── Services (Auth, Personalization, Translation)
        │   └── Config (Settings, Agent definitions)
        │
        ├─→ Neon Postgres (User data, preferences, chat history)
        ├─→ Qdrant Vector DB (RAG Embeddings)
        └─→ GEMINI API (Agents SDK for chat completions)
```

### Request Flow

```
1. User Query → Frontend ChatWidget
                    ↓
2. POST /api/v1/chat → FastAPI Router
                    ↓
3. Agent Orchestrator analyzes query intent
   - Classifies: concept | code | troubleshoot | learning
                    ↓
4. Routes to appropriate agent
   - Concept Explainer Agent
   - Code Helper Agent
   - Troubleshooting Agent
                    ↓
5. Agent calls RAG Service
   - Generate query embedding (Gemini)
   - Search Qdrant vector DB
   - Retrieve top 5 relevant chapters
                    ↓
6. Context Builder assembles context
   - Format chapter excerpts
   - Add metadata (chapter IDs, titles)
   - Truncate to token limit
                    ↓
7. Agent generates response (OpenAI Agents SDK)
   - System prompt defines agent persona
   - Context injected into prompt
   - Generate educational response
                    ↓
8. Response returned to user
   - Answer text
   - Source citations
   - Agent name
   - Performance metrics
```

---

## Updated Backend Directory Structure

```
backend/
├── app/
│   ├── main.py                         # FastAPI application entry
│   │
│   ├── config/                         # Configuration module
│   │   ├── __init__.py
│   │   ├── settings.py                 # Environment variables, API keys
│   │   └── agents_config.py            # Agent definitions, prompts, routing rules
│   │
│   ├── routes/                         # API routes (previously api/v1/endpoints/)
│   │   ├── __init__.py
│   │   ├── chat.py                     # POST /chat (agent orchestration)
│   │   ├── auth.py                     # POST /signup, /login, /refresh
│   │   ├── personalization.py          # POST /personalize
│   │   └── translation.py              # POST /translate
│   │
│   ├── schemas/                        # Pydantic models (API contracts)
│   │   ├── __init__.py
│   │   ├── chat.py                     # ChatRequest, ChatResponse, AgentTask
│   │   ├── auth.py                     # UserCreate, UserResponse, TokenResponse
│   │   ├── personalization.py          # PersonalizationRequest, PersonalizationResponse
│   │   └── translation.py              # TranslationRequest, TranslationResponse
│   │
│   ├── agents/                         # OpenAI Agents SDK implementation
│   │   ├── __init__.py
│   │   ├── orchestrator.py             # Agent task routing logic
│   │   ├── concept_agent.py            # Concept Explainer Agent
│   │   ├── code_agent.py               # Code Helper Agent
│   │   ├── troubleshoot_agent.py       # Troubleshooting Agent
│   │   └── learning_agent.py           # Learning Path Agent (bonus)
│   │
│   ├── chatbot/                        # RAG services (previously services/chatbot/)
│   │   ├── __init__.py
│   │   ├── rag_service.py              # RAG orchestration (main entry point)
│   │   ├── vector_search.py            # Qdrant vector similarity search
│   │   ├── embedder.py                 # Gemini embedding generation
│   │   └── context_builder.py          # Context assembly for agents
│   │
│   ├── services/                       # Business logic services
│   │   ├── __init__.py
│   │   ├── auth_service.py             # Authentication logic
│   │   ├── personalization_service.py  # Content adaptation
│   │   └── translation_service.py      # Urdu translation
│   │
│   ├── db/                             # Database layer
│   │   ├── __init__.py
│   │   ├── session.py                  # Database session management
│   │   └── models/
│   │       ├── __init__.py
│   │       ├── user.py                 # User model
│   │       ├── preference.py           # UserPreference model
│   │       └── chat_history.py         # ChatHistory model (updated with agent metadata)
│   │
│   └── utils/                          # Utility functions
│       ├── __init__.py
│       ├── logging.py                  # Structured logging
│       └── validators.py               # Custom Pydantic validators
│
├── scripts/                            # Setup and maintenance scripts
│   ├── seed_vector_db.py               # Populate Qdrant with chapter embeddings
│   ├── test_connections.py             # Test external API connections
│   └── test_agents.py                  # Test agent routing and responses
│
├── tests/                              # Test suite
│   ├── unit/
│   │   ├── test_orchestrator.py        # Agent routing logic tests
│   │   ├── test_agents.py              # Individual agent tests
│   │   └── test_rag_service.py         # RAG pipeline tests
│   ├── integration/
│   │   ├── test_chat_api.py            # End-to-end chat tests
│   │   └── test_agent_rag.py           # Agent + RAG integration tests
│   └── conftest.py                     # Pytest fixtures
│
├── requirements/
│   ├── base.txt                        # Core dependencies
│   ├── dev.txt                         # Development tools
│   └── prod.txt                        # Production dependencies
│
├── alembic/                            # Database migrations
│   ├── versions/
│   └── env.py
│
├── .env.example                        # Environment variable template
├── pyproject.toml                      # Poetry configuration
└── Dockerfile                          # Container for deployment
```

---

## Key Architecture Decisions (ADRs)

### ADR-001: OpenAI Agents SDK for Multi-Agent Architecture

**Context**: Phase 3 complete. Need to implement chatbot with better specialization than single LLM.

**Decision**: Use OpenAI Agents SDK to create specialized agents (Concept, Code, Troubleshoot) that handle different query types.

**Alternatives Considered**:
1. **Single GPT-4 model with prompt engineering**: 
   - Pro: Simpler architecture
   - Con: Less specialization, harder to maintain distinct personas
   
2. **LangChain agents**:
   - Pro: More flexible, many tools
   - Con: Heavier framework, more complexity
   
3. **OpenAI Agents SDK**:
   - Pro: Native OpenAI integration, clean API, task specialization
   - Con: Newer, less mature than LangChain

**Rationale**: 
- OpenAI Agents SDK provides clean task-based architecture
- Each agent has distinct system prompt and task definition
- Better separation of concerns (orchestrator routes, agents specialize)
- Easier to extend (add new agents without changing core logic)

**Consequences**:
- **Positive**: Clear agent specialization, easier debugging, better UX (users see which agent helped them)
- **Negative**: Additional routing layer (orchestrator), slightly more complex than single LLM
- **Constraint**: Future agents must follow SDK patterns

---

### ADR-002: Backend Structure Reorganization

**Context**: Original structure had `api/v1/endpoints/`, `services/chatbot/`. With multi-agent architecture, need clearer separation.

**Decision**: Restructure backend as:
- `routes/` (API endpoints)
- `agents/` (OpenAI Agents SDK implementation)
- `chatbot/` (RAG services)
- `config/` (all configuration including agent definitions)

**Rationale**:
- `routes/` is more standard than `api/v1/endpoints/`
- `agents/` clearly separates agent logic from RAG services
- `chatbot/` focuses purely on RAG (embeddings, search, context)
- `config/` centralizes all settings (environment, agent prompts)

**Consequences**:
- **Positive**: Clearer separation of concerns, easier to locate code
- **Negative**: Requires refactoring if Phase 3 used old structure
- **Constraint**: All new features follow this structure

---

## Implementation Phases (Updated)

### Phase 1: Foundation Setup ✅ (Completed)

Already completed. No changes needed.

---

### Phase 2: Book Content Creation ✅ (Completed)

Already completed. No changes needed.

---

### Phase 3: RAG Backend ✅ (Completed - Now Enhanced)

**Status**: Core RAG functionality complete. Now enhancing with agent architecture.

**What was completed**:
- Embeddings script (reads MDX → Qdrant)
- Vector search service
- Context builder
- Basic RAG pipeline

**What needs updating**:
- Refactor into new `chatbot/` directory structure
- Expose RAG as service for agents to call
- Add agent-specific context filtering

---

### Phase 3.5: Multi-Agent System (NEW - Current Phase)

**Goal**: Implement OpenAI Agents SDK with specialized agents and orchestrator.

**Deliverables**:
- Agent orchestrator (query classification and routing)
- 3 specialized agents (Concept, Code, Troubleshoot)
- Agent-RAG integration
- Updated chat API endpoint

**Key Activities**:

#### Task 3.5.1: Setup OpenAI Agents SDK
**Duration**: 30 minutes  
**Type**: Setup

**What to do**:
- Install OpenAI Agents SDK: `pip install openai-agents-sdk`
- Create `config/agents_config.py` with agent definitions
- Define agent system prompts and task keywords
- Test basic agent creation

**Acceptance Criteria**:
- OpenAI Agents SDK installed
- `agents_config.py` has definitions for 3 agents
- Can instantiate agent from config

---

#### Task 3.5.2: Implement Agent Orchestrator
**Duration**: 60 minutes  
**Type**: Feature Implementation

**What to do**:
- Create `agents/orchestrator.py`
- Implement query classification logic:
  - Extract keywords from user query
  - Match against agent task keywords
  - Return agent name + confidence score
- Handle ambiguous queries (default to Concept Explainer)
- Add logging for routing decisions

**Acceptance Criteria**:
- Orchestrator classifies queries correctly (>80% accuracy on test set)
- Returns agent name and confidence score
- Handles edge cases (empty query, ambiguous query)

**Test Cases**:
```python
assert orchestrator.route("What is ROS 2?") == ("concept_explainer", 0.9)
assert orchestrator.route("Show me code for publisher") == ("code_helper", 0.85)
assert orchestrator.route("Error installing Gazebo") == ("troubleshoot", 0.88)
```

---

#### Task 3.5.3: Implement Concept Explainer Agent
**Duration**: 45 minutes  
**Type**: Agent Implementation

**What to do**:
- Create `agents/concept_agent.py`
- Load agent config from `agents_config.py`
- Implement `process_query(query, rag_context)` method:
  - Takes user query + RAG context
  - Calls OpenAI Agents SDK with system prompt
  - Returns educational explanation
- Add source citation logic
- Test with sample queries

**Acceptance Criteria**:
- Agent generates educational explanations
- Uses RAG context in responses
- Cites source chapters
- Tone matches "patient educator" persona

---

#### Task 3.5.4: Implement Code Helper Agent
**Duration**: 45 minutes  
**Type**: Agent Implementation

**What to do**:
- Create `agents/code_agent.py`
- Similar structure to Concept Agent
- Focus on code examples and best practices
- Validate code snippets (syntax check if possible)

**Acceptance Criteria**:
- Agent provides working code examples
- Explains code line-by-line
- Focuses on practical solutions
- Tone matches "experienced developer" persona

---

#### Task 3.5.5: Implement Troubleshooting Agent
**Duration**: 45 minutes  
**Type**: Agent Implementation

**What to do**:
- Create `agents/troubleshoot_agent.py`
- Implement diagnostic questioning approach
- Format responses as step-by-step solutions
- Link to troubleshooting sections from book

**Acceptance Criteria**:
- Agent asks diagnostic questions
- Provides systematic debugging steps
- Tone matches "senior engineer" persona
- References troubleshooting documentation

---

#### Task 3.5.6: Integrate Agents with RAG
**Duration**: 60 minutes  
**Type**: Integration

**What to do**:
- Refactor `chatbot/rag_service.py` to expose clean API for agents
- Add agent-specific RAG filters (e.g., Code Agent prioritizes code examples)
- Update context builder to format for agent consumption
- Test agent + RAG integration

**Acceptance Criteria**:
- Agents can call RAG service
- RAG returns filtered context based on agent type
- Context format optimized for agent prompts

---

#### Task 3.5.7: Update Chat API Endpoint
**Duration**: 45 minutes  
**Type**: API Update

**What to do**:
- Update `routes/chat.py` to use agent orchestrator
- Flow: Request → Orchestrator → Agent → RAG → Response
- Add agent metadata to response (agent_used, confidence)
- Add performance metrics (rag_time_ms, agent_time_ms)
- Update response schema

**Acceptance Criteria**:
- POST /chat uses orchestrator + agents
- Response includes agent name and metadata
- Performance metrics logged
- Error handling for agent failures

---

### Phase 4: Chatbot Frontend ✅ (Completed - Minor Updates)

**Status**: Core chatbot UI complete. Small updates needed.

**Updates Required**:
- Display agent name in chat response (e.g., "Code Helper Agent")
- Show agent icon/avatar (optional visual enhancement)
- Add agent switching indicator ("Connecting you to Troubleshooting Agent...")

---

### Phase 5-10: No Changes

Phases 5 (Authentication), 6 (Personalization), 7 (Translation), 8 (Testing), 9 (Deployment), 10 (Reusable Intelligence) remain unchanged.

---

## Configuration File Structure

### config/settings.py

```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # API Keys
    GEMINI_API_KEY: str
    GEMINI_BASE_URL: str
    QDRANT_URL: str
    QDRANT_API_KEY: str
    NEON_DATABASE_URL: str
    
    # JWT Settings
    JWT_SECRET_KEY: str
    JWT_ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 15
    REFRESH_TOKEN_EXPIRE_DAYS: int = 7
    
    # Qdrant Settings
    QDRANT_COLLECTION_NAME: str = "book_chapters_en"
    QDRANT_VECTOR_SIZE: int = 3072  # gemini-embedding-001
    
    # OpenAI Settings
    OPENAI_CHAT_MODEL: str = "gpt-4-turbo"
    OPENAI_MAX_TOKENS: int = 800
    OPENAI_TEMPERATURE: float = 0.3
    
    # RAG Settings
    RAG_TOP_K: int = 5
    RAG_SCORE_THRESHOLD: float = 0.6
    RAG_MAX_CONTEXT_TOKENS: int = 6000
    
    # CORS Settings
    CORS_ORIGINS: list = ["http://localhost:3000"]
    
    class Config:
        env_file = ".env"
        case_sensitive = True

@lru_cache()
def get_settings():
    return Settings()
```

### config/agents_config.py

```python
AGENTS_CONFIG = {
    "concept_explainer": {
        "name": "Concept Explainer Agent",
        "description": "Explains theoretical concepts using analogies and clear language",
        "system_prompt": """You are a patient educator specializing in Physical AI and Robotics.

Your role:
- Explain theoretical concepts clearly using analogies
- Break down complex topics into digestible parts
- Focus on "why" and "how things work"
- Use the provided RAG context from book chapters

Style:
- Patient and encouraging
- Use real-world analogies
- Progressive complexity (simple → advanced)
- Always cite source chapters

Context format:
You will receive relevant chapter excerpts in this format:
[Chapter: module-1-ros2/01-introduction]
{content}

Always reference these sources in your explanation.""",
        
        "task_keywords": [
            "explain", "what is", "how does", "why", "concept", 
            "theory", "understand", "meaning", "definition"
        ],
        
        "rag_filter_keywords": [
            "theory", "concepts", "introduction", "overview", 
            "fundamentals", "basics"
        ],
        
        "confidence_threshold": 0.7
    },
    
    "code_helper": {
        "name": "Code Helper Agent",
        "description": "Assists with coding questions, debugging, and best practices",
        "system_prompt": """You are an experienced robotics developer specializing in ROS 2, Gazebo, and NVIDIA Isaac.

Your role:
- Help debug code issues
- Explain code examples from the book
- Suggest best practices
- Provide working code snippets

Style:
- Direct and practical
- Focus on working solutions
- Explain "why" code works this way
- Always cite source chapters with code examples

Code formatting:
- Use proper syntax highlighting
- Include comments for clarity
- Provide complete, runnable examples
- Explain each significant line

Context format:
You will receive code examples and explanations from chapters.
Reference these directly and build upon them.""",
        
        "task_keywords": [
            "code", "debug", "error", "implement", "example", 
            "syntax", "function", "class", "how to code", "programming"
        ],
        
        "rag_filter_keywords": [
            "code", "example", "implementation", "snippet", 
            "function", "class", "method"
        ],
        
        "confidence_threshold": 0.75
    },
    
    "troubleshoot": {
        "name": "Troubleshooting Agent",
        "description": "Helps diagnose and resolve technical issues",
        "system_prompt": """You are a senior robotics engineer specializing in debugging and system issues.

Your role:
- Help diagnose installation problems
- Debug runtime errors
- Resolve environment issues
- Guide systematic troubleshooting

Approach:
1. Ask clarifying questions about the error
2. Identify likely causes
3. Provide step-by-step solutions
4. Suggest preventive measures

Style:
- Systematic and methodical
- Clear diagnostic questions
- Step-by-step solutions
- Reference troubleshooting sections from book

Context format:
You will receive troubleshooting guides and common issues from chapters.
Use these as reference for your diagnostic approach.""",
        
        "task_keywords": [
            "error", "problem", "doesn't work", "issue", "install", 
            "fix", "broken", "crash", "fails", "troubleshoot"
        ],
        
        "rag_filter_keywords": [
            "troubleshooting", "common issues", "setup", "installation",
            "error", "debugging", "problems"
        ],
        
        "confidence_threshold": 0.8
    }
}

# Routing configuration
ROUTING_CONFIG = {
    "default_agent": "concept_explainer",  # Fallback if no strong match
    "min_confidence": 0.6,  # Minimum confidence to route to specific agent
    "keyword_match_weight": 0.7,  # Weight for keyword matching
    "context_weight": 0.3  # Weight for contextual analysis
}
```

---

## Updated Dependency Graph

```
Phase 1 (Foundation) ✅
    ├──→ Phase 2 (Book Content) ✅
    │                   ↓
    └──→ Phase 3 (RAG Backend) ✅
                   ↓
         Phase 3.5 (Multi-Agent System) ← CURRENT
                   ↓
    ┌──────────────┴──────────────┐
    ▼                             ▼
Phase 4 (Chatbot Frontend)  Phase 5 (Authentication)
    │                             │
    └──────────────┬──────────────┘
                   ↓
         Phase 6 (Personalization)
         Phase 7 (Translation)
                   ↓
         Phase 8 (Testing)
                   ↓
         Phase 9 (Deployment)

Phase 10 (Reusable Intelligence) → Parallel to all phases
```

**Critical Path**: Phase 1 → 2 → 3 → 3.5 → 4 → 8 → 9 (Base deliverable)

---

## Estimated Time (Updated)

**Phase 3.5 (Multi-Agent System)**: 6-8 hours
- Task 3.5.1: Setup (30 min)
- Task 3.5.2: Orchestrator (60 min)
- Task 3.5.3: Concept Agent (45 min)
- Task 3.5.4: Code Agent (45 min)
- Task 3.5.5: Troubleshoot Agent (45 min)
- Task 3.5.6: Agent-RAG Integration (60 min)
- Task 3.5.7: API Update (45 min)
- Testing & debugging: 90 min

**Total Project (with multi-agent)**: 70-90 hours for base deliverable

---

## Success Metrics (Updated)

**Phase 3.5 Completion Criteria**:
- [ ] 3 specialized agents implemented
- [ ] Orchestrator routes queries correctly (>80% accuracy)
- [ ] Agents use RAG context before generating responses
- [ ] API response includes agent metadata
- [ ] Response time P95 < 3 seconds (including routing + RAG + generation)
- [ ] Agent responses demonstrate distinct personas

**Quality Gates**:
- Agent routing accuracy: >80%
- RAG context relevance: >0.6 average
- Response time: <3s (P95)
- Test coverage: >75% for agent logic

---

## Revision History

| Version | Date       | Changes                                      | Author |
|---------|------------|----------------------------------------------|--------|
| 1.0.0   | 2024-12-06 | Initial implementation plan                  | AI     |
| 2.0.0   | 2024-12-06 | Multi-agent architecture with OpenAI SDK     | AI     |