# Implementation Tasks: Physical AI Book with RAG Chatbot

**Version**: 1.0.0  
**Status**: Active  
**Created**: 2024-12-06  
**Dependencies**: plan.md

---

## Task Breakdown Structure

### Phase 1: Foundation Setup

#### Task 1.1: Initialize Project Structure
**Duration**: 30 minutes  
**Dependencies**: None  
**Type**: Setup  

**What to do**:
- Create monorepo root directory
- Run `npx create-docusaurus@latest frontend classic --typescript`
- Create `backend/` directory with FastAPI structure
- Create `.specify/`, `specs/`, `history/` directories
- Create `.gitignore` (exclude .env, node_modules, __pycache__)

**Acceptance Criteria**:
- Directory structure matches plan.md
- `frontend/` has working Docusaurus installation
- `backend/` has main.py with FastAPI() instance
- Git repository initialized

**Validation**:
```bash
cd frontend && npm start  # Should open Docusaurus
cd backend && uvicorn app.main:app --reload  # Should show FastAPI docs
```

---

#### Task 1.2: Configure Environment Variables
**Duration**: 20 minutes  
**Dependencies**: Task 1.1  
**Type**: Configuration  

**What to do**:
- Create `backend/.env.example` with template:
  ```
  OPENAI_API_KEY=sk-proj-...
  QDRANT_URL=https://...
  QDRANT_API_KEY=...
  NEON_DATABASE_URL=postgres://...
  JWT_SECRET_KEY=generate-random-key
  ```
- Create `frontend/.env.example`:
  ```
  REACT_APP_API_URL=http://localhost:8000/api/v1
  ```
- Add instructions in README for creating `.env` files

**Acceptance Criteria**:
- `.env.example` files created (no actual secrets)
- README documents how to configure environment
- `.env` added to .gitignore

---

#### Task 1.3: Test External API Connections
**Duration**: 30 minutes  
**Dependencies**: Task 1.2  
**Type**: Validation  

**What to do**:
- Write test script `backend/scripts/test_connections.py`:
  - Test OpenAI API (generate sample embedding)
  - Test Qdrant connection (list collections)
  - Test Neon Postgres (execute simple query)
- Run script, verify all connections succeed

**Acceptance Criteria**:
- Script runs without errors
- OpenAI returns 1536-dimensional embedding
- Qdrant connection successful
- Neon query returns result

---

#### Task 1.4: Setup Development Documentation
**Duration**: 30 minutes  
**Dependencies**: Task 1.1, 1.2, 1.3  
**Type**: Documentation  

**What to do**:
- Create `README.md` with:
  - Project overview
  - Prerequisites (Node.js 18+, Python 3.12+)
  - Setup instructions (clone â†’ install â†’ configure .env â†’ run)
  - Available scripts (npm start, uvicorn)
  - Troubleshooting common issues
- Create `CONTRIBUTING.md` with:
  - Git workflow (branch naming, commit messages)
  - Code style guide (ESLint, Black)
  - Testing requirements

**Acceptance Criteria**:
- New developer can follow README and set up project in < 30 minutes
- Documentation is clear and complete

---

### Phase 2: Book Content Creation

#### Task 2.1: Research Module 1 Content (ROS 2)
**Duration**: 45 minutes  
**Dependencies**: Task 1.1  
**Type**: Research  

**What to do**:
- Use Context7 MCP to query official ROS 2 documentation
- Research topics:
  - ROS 2 architecture (nodes, topics, services, actions)
  - rclpy Python API
  - Message types (std_msgs, geometry_msgs)
  - Quality of Service (QoS) settings
- Take notes on key concepts, examples

**Acceptance Criteria**:
- Notes cover 4 chapter topics
- Official ROS 2 docs referenced
- Code examples identified

**Gemini CLI Prompt**:
```
Use Context7 MCP to research ROS 2 fundamentals:
1. What are ROS 2 nodes, topics, services, actions?
2. How to create a node with rclpy?
3. What are common message types?
4. How does QoS work?

Provide: Concept explanations + code examples + best practices
```

---

#### Task 2.2: Write Module 1, Chapter 1 (ROS 2 Introduction)
**Duration**: 60 minutes  
**Dependencies**: Task 2.1  
**Type**: Content Creation  

**What to do**:
- Create `frontend/docs/module-1-ros2/01-introduction.md`
- Structure:
  - What is ROS 2? (middleware, history, why upgrade from ROS 1)
  - Core concepts (nodes, topics, services, actions)
  - Installation (link to official guide)
  - Your first "Hello World" node (Python)
- Include:
  - Mermaid diagram (ROS 2 architecture)
  - Complete Python example (runnable)
  - Exercises for practice

**Acceptance Criteria**:
- Chapter 1000-1500 words
- Code example tested (runs on ROS 2 Humble)
- Mermaid diagram renders correctly
- No spelling/grammar errors

**Gemini CLI Prompt**:
```
Write Chapter 1: ROS 2 Introduction

Target audience: Beginners with Python experience
Structure: Problem â†’ Concept â†’ Example â†’ Exercise
Tone: Educational, clear, encouraging

Include:
- What is ROS 2 and why it exists
- Core architecture (nodes, topics, services)
- First "Hello World" node (rclpy)
- Mermaid diagram of node communication

Length: 1000-1500 words
Format: MDX (Docusaurus)
```

---

#### Task 2.3: Write Module 1, Chapter 2-4
**Duration**: 180 minutes (60 min each)  
**Dependencies**: Task 2.2  
**Type**: Content Creation  

**What to do**:
- Chapter 2: Building Your First Node
- Chapter 3: Topics and Pub/Sub Pattern
- Chapter 4: Services and Actions

**Acceptance Criteria**:
- Each chapter 1000-1500 words
- All code examples tested
- Follows same structure as Chapter 1

---

#### Task 2.4: Write Module 2 (Gazebo Simulation)
**Duration**: 180 minutes  
**Dependencies**: Task 2.3  
**Type**: Content Creation  

**What to do**:
- Chapter 1: Gazebo Environment Setup
- Chapter 2: URDF Robot Modeling
- Chapter 3: Physics and Sensors

**Acceptance Criteria**:
- 3 chapters, each 1000-1500 words
- URDF examples included
- Simulation tutorials tested

---

#### Task 2.5: Write Module 3 (NVIDIA Isaac)
**Duration**: 180 minutes  
**Dependencies**: Task 2.4  
**Type**: Content Creation  

**What to do**:
- Chapter 1: Isaac Sim Introduction
- Chapter 2: Isaac ROS Perception
- Chapter 3: Sim-to-Real Transfer

**Acceptance Criteria**:
- 3 chapters complete
- USD format explained
- Perception pipeline examples

---

#### Task 2.6: Write Module 4 (VLA)
**Duration**: 120 minutes  
**Dependencies**: Task 2.5  
**Type**: Content Creation  

**What to do**:
- Chapter 1: Voice Commands (Whisper + LLM)
- Chapter 2: Cognitive Planning

**Acceptance Criteria**:
- 2 chapters complete
- Voice-to-action examples
- LLM integration explained

---

#### Task 2.7: Configure Docusaurus Navigation
**Duration**: 20 minutes  
**Dependencies**: Task 2.6  
**Type**: Configuration  

**What to do**:
- Edit `frontend/sidebars.ts` to structure modules/chapters
- Configure `docusaurus.config.ts` (site title, base URL)
- Test navigation (all links work)

**Acceptance Criteria**:
- Sidebar shows all 4 modules with chapters
- Navigation functional
- Mobile responsive

---

### Phase 3: RAG Chatbot Backend

#### Task 3.1: Implement Embeddings Script
**Duration**: 60 minutes  
**Dependencies**: Task 2.7, Task 1.3  
**Type**: Feature Implementation  

**What to do**:
- Create `backend/scripts/seed_vector_db.py`
- Logic:
  - Read all MDX files from frontend/docs/
  - Parse content (remove frontmatter)
  - Chunk text (500-1000 tokens per chunk)
  - Generate embeddings (gemini-embedding-001)
  - Upload to Qdrant collection `book_chapters_en` for RAG

**Acceptance Criteria**:
- Script runs successfully
- Qdrant collection created with N vectors (N = total chunks)
- Payload includes: chapter_id, section_title, content, chunk_index

**Validation**:
```bash
python backend/scripts/seed_vector_db.py
# Check Qdrant dashboard: collection "book_chapters_en" exists with vectors
```

---

#### Task 3.2: Implement Vector Search Service
**Duration**: 45 minutes  
**Dependencies**: Task 3.1  
**Type**: Feature Implementation  

**What to do**:
- Create `backend/app/services/chatbot/vector_search.py`
- Class: `VectorSearchService`
- Method: `search(query_vector, limit=5, score_threshold=0.6)`
- Logic:
  - Query Qdrant with cosine similarity
  - Filter by score > threshold
  - Return top K results with metadata

**Acceptance Criteria**:
- Service queries Qdrant successfully
- Returns results sorted by relevance
- Handles empty results (returns [])

---

#### Task 3.3: Implement Context Builder
**Duration**: 45 minutes  
**Dependencies**: Task 3.2  
**Type**: Feature Implementation  

**What to do**:
- Create `backend/app/services/chatbot/context_builder.py`
- Class: `ContextBuilder`
- Method: `build(search_results, selected_text, conversation_history, max_tokens=6000)`
- Logic:
  - Priority: selected text > conversation history > search results
  - Format as: `[Source 1]\n{content}\n---\n[Source 2]...`
  - Truncate if exceeds token limit

**Acceptance Criteria**:
- Context string formatted correctly
- Token limit respected (use tiktoken)
- Priority ordering works

---

#### Task 3.4: Implement RAG Engine
**Duration**: 60 minutes  
**Dependencies**: Task 3.3  
**Type**: Feature Implementation  

**What to do**:
- Create `backend/app/services/chatbot/rag_engine.py`
- Class: `RAGEngine`
- Method: `process_query(message, context, conversation_history)`
- Logic:
  - Generate query embedding
  - Search vector DB
  - Build context
  - Call OpenAI Agents SDK for chat completion

**Acceptance Criteria**:
- Full pipeline works end-to-end
- Handles errors (API limits, no results)
- Returns structured response

---

#### Task 3.5: Create Chat API Endpoint
**Duration**: 30 minutes  
**Dependencies**: Task 3.4  
**Type**: API Development  

**What to do**:
- Create `backend/app/api/v1/endpoints/chatbot.py`
- Endpoint: `POST /api/v1/chat`
- Request schema: `ChatRequest(message, context, conversation_history)`
- Response schema: `ChatResponse(answer, sources, confidence)`
- Logic:
  - Validate request (Pydantic)
  - Call RAG engine
  - Return response

**Acceptance Criteria**:
- Endpoint accessible at POST /api/v1/chat
- Request validation works
- Returns 200 with valid response

**Validation**:
```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is ROS 2?"}'
# Should return JSON with answer and sources
```

---

### Phase 4: Chatbot Frontend

#### Task 4.1: Create ChatWidget Component
**Duration**: 60 minutes  
**Dependencies**: Task 3.5  
**Type**: UI Development  

**What to do**:
- Create `frontend/src/components/Chatbot/ChatWidget.tsx`
- Features:
  - Floating button (bottom-right)
  - Expandable chat panel
  - Message list (user + assistant)
  - Input field + send button
  - Loading indicator

**Acceptance Criteria**:
- Component renders correctly
- Button toggles panel open/closed
- Messages display in correct order

---

#### Task 4.2: Implement useChatbot Hook
**Duration**: 45 minutes  
**Dependencies**: Task 4.1  
**Type**: Logic Implementation  

**What to do**:
- Create `frontend/src/hooks/useChatbot.ts`
- Features:
  - State management (messages, loading, error)
  - API client (calls POST /chat)
  - Error handling (retry logic)

**Acceptance Criteria**:
- Hook manages chat state correctly
- API calls succeed
- Errors caught and displayed

---

#### Task 4.3: Implement Selected-Text Feature
**Duration**: 30 minutes  
**Dependencies**: Task 4.2  
**Type**: Feature Enhancement  

**What to do**:
- Detect text selection (window.getSelection())
- Keyboard shortcut (Ctrl+Q or Cmd+Q)
- Pass selected text as context to API

**Acceptance Criteria**:
- Text selection detected
- Keyboard shortcut works
- Context passed to API correctly

---

#### Task 4.4: Style ChatWidget
**Duration**: 45 minutes  
**Dependencies**: Task 4.1  
**Type**: Styling  

**What to do**:
- Create `frontend/src/components/Chatbot/ChatWidget.css`
- Responsive design (desktop + mobile)
- Match Docusaurus theme colors
- Accessibility (keyboard navigation, focus indicators)

**Acceptance Criteria**:
- Mobile responsive (works on 375px width)
- Colors match theme
- WCAG 2.1 AA compliant

---

### Phase 5: Authentication System

#### Task 5.1: Setup Better-auth Backend
**Duration**: 60 minutes  
**Dependencies**: Task 1.3  
**Type**: Integration  

**What to do**:
- Install better-auth (or equivalent library)
- Create `users` table in Neon (migration)
- Configure JWT settings (15-min access, 7-day refresh)
- Implement password hashing (bcrypt)

**Acceptance Criteria**:
- Better-auth configured
- Database schema created
- JWT tokens generated correctly

---

#### Task 5.2: Create Signup Endpoint
**Duration**: 45 minutes  
**Dependencies**: Task 5.1  
**Type**: API Development  

**What to do**:
- Endpoint: `POST /api/v1/auth/signup`
- Request: `{email, password, full_name, software_background, hardware_background}`
- Logic:
  - Validate email format, password strength
  - Hash password
  - Insert user into database
  - Return JWT tokens

**Acceptance Criteria**:
- Endpoint validates input correctly
- Passwords hashed (never stored plain-text)
- Returns access + refresh tokens

---

#### Task 5.3: Create Login Endpoint
**Duration**: 30 minutes  
**Dependencies**: Task 5.1  
**Type**: API Development  

**What to do**:
- Endpoint: `POST /api/v1/auth/login`
- Request: `{email, password}`
- Logic:
  - Verify credentials
  - Return JWT tokens
  - Update last_login timestamp

**Acceptance Criteria**:
- Endpoint verifies credentials
- Returns tokens on success
- Returns 401 on invalid credentials

---

#### Task 5.4: Create Auth UI (Frontend)
**Duration**: 60 minutes  
**Dependencies**: Task 5.2, 5.3  
**Type**: UI Development  

**What to do**:
- Create `LoginModal` component (signup/login forms)
- Create `ProfileButton` component (user menu in navbar)
- Implement token storage (localStorage or HttpOnly cookies)

**Acceptance Criteria**:
- Forms validate input
- API calls succeed
- Tokens stored securely

---

#### Task 5.5: Implement Token Refresh
**Duration**: 30 minutes  
**Dependencies**: Task 5.4  
**Type**: Logic Implementation  

**What to do**:
- Axios interceptor catches 401 responses
- Attempts token refresh
- Retries original request

**Acceptance Criteria**:
- Expired tokens refreshed automatically
- Original request succeeds after refresh

---

### Phase 6: Personalization Engine

#### Task 6.1: Design Personalization Prompt
**Duration**: 30 minutes  
**Dependencies**: Task 5.5  
**Type**: Prompt Engineering  

**What to do**:
- Create prompt template for content adaptation
- Test with sample chapter + user background
- Iterate until output quality is good

**Acceptance Criteria**:
- Prompt adapts content correctly
- Technical accuracy maintained
- Examples match user level

---

#### Task 6.2: Implement Personalization Service
**Duration**: 45 minutes  
**Dependencies**: Task 6.1  
**Type**: Feature Implementation  

**What to do**:
- Create `backend/app/services/personalization_service.py`
- Method: `personalize_content(chapter_content, user_background, level)`
- Logic:
  - Call OpenAI Agents SDK with personalization prompt
  - Return adapted markdown

**Acceptance Criteria**:
- Service adapts content correctly
- Response time < 5 seconds

---

#### Task 6.3: Create Personalization API Endpoint
**Duration**: 30 minutes  
**Dependencies**: Task 6.2  
**Type**: API Development  

**What to do**:
- Endpoint: `POST /api/v1/personalize`
- Request: `{chapter_id, personalization_level}`
- Requires authentication

**Acceptance Criteria**:
- Endpoint requires auth
- Returns personalized markdown

---

#### Task 6.4: Create PersonalizeButton Component
**Duration**: 45 minutes  
**Dependencies**: Task 6.3  
**Type**: UI Development  

**What to do**:
- Create button visible only when authenticated
- Dropdown: Beginner / Intermediate / Advanced
- Calls API, replaces chapter content
- "Reset to Original" button

**Acceptance Criteria**:
- Button appears for logged-in users
- Personalization works correctly
- Reset button restores original

---

### Phase 7: Translation System

#### Task 7.1: Design Translation Prompt
**Duration**: 30 minutes  
**Dependencies**: None  
**Type**: Prompt Engineering  

**What to do**:
- Create prompt template for Urdu translation
- Rules: Keep technical terms in English, maintain formatting
- Test with sample chapter

**Acceptance Criteria**:
- Translation quality is good
- Technical terms preserved
- Formatting maintained

---

#### Task 7.2: Implement Translation Service
**Duration**: 45 minutes  
**Dependencies**: Task 7.1  
**Type**: Feature Implementation  

**What to do**:
- Create `backend/app/services/translation_service.py`
- Method: `translate_content(chapter_content, target_language)`
- Logic:
  - Call OpenAI Agents SDK with translation prompt
  - Return translated markdown

**Acceptance Criteria**:
- Service translates correctly
- Response time < 5 seconds

---

#### Task 7.3: Create Translation API Endpoint
**Duration**: 30 minutes  
**Dependencies**: Task 7.2  
**Type**: API Development  

**What to do**:
- Endpoint: `POST /api/v1/translate`
- Request: `{chapter_id, target_language}`

**Acceptance Criteria**:
- Endpoint returns translated markdown

---

#### Task 7.4: Create TranslateButton Component
**Duration**: 45 minutes  
**Dependencies**: Task 7.3  
**Type**: UI Development  

**What to do**:
- Create button to toggle English/Urdu
- Apply RTL layout (CSS direction: rtl)
- Code blocks remain LTR

**Acceptance Criteria**:
- Translation works correctly
- RTL layout applied
- Code blocks unchanged

---

### Phase 8: Testing & QA

#### Task 8.1: Write Backend Unit Tests
**Duration**: 90 minutes  
**Dependencies**: All backend tasks  
**Type**: Testing  

**What to do**:
- Test RAG pipeline components
- Test auth service (signup, login)
- Test personalization service
- Target: 80% coverage

**Acceptance Criteria**:
- pytest runs successfully
- Coverage > 80%

---

#### Task 8.2: Write Frontend Component Tests
**Duration**: 90 minutes  
**Dependencies**: All frontend tasks  
**Type**: Testing  

**What to do**:
- Test ChatWidget interactions
- Test auth forms
- Test personalization button
- Target: 70% coverage

**Acceptance Criteria**:
- Jest tests pass
- Coverage > 70%

---

#### Task 8.3: Run Lighthouse Audit
**Duration**: 30 minutes  
**Dependencies**: All frontend tasks  
**Type**: Quality Check  

**What to do**:
- Run Lighthouse on deployed site
- Fix issues to meet targets:
  - Performance > 85
  - Accessibility 100
  - Best Practices > 95

**Acceptance Criteria**:
- Lighthouse scores meet targets

---

#### Task 8.4: Security Audit
**Duration**: 30 minutes  
**Dependencies**: All tasks  
**Type**: Security Check  

**What to do**:
- Run `npm audit` and fix vulnerabilities
- Run `safety check` on Python dependencies
- Manual review for SQL injection, XSS, CSRF

**Acceptance Criteria**:
- Zero critical/high vulnerabilities

---

### Phase 9: Deployment

#### Task 9.1: Deploy Frontend to GitHub Pages
**Duration**: 30 minutes  
**Dependencies**: Task 8.3  
**Type**: Deployment  

**What to do**:
- Configure GitHub Actions workflow
- Build Docusaurus
- Deploy to gh-pages branch

**Acceptance Criteria**:
- Site accessible at GitHub Pages URL

---

#### Task 9.2: Deploy Backend to Railway
**Duration**: 45 minutes  
**Dependencies**: Task 8.4  
**Type**: Deployment  

**What to do**:
- Connect GitHub repo to Railway
- Configure environment variables
- Deploy backend

**Acceptance Criteria**:
- Backend accessible at Railway URL
- All endpoints work in production

---

#### Task 9.3: Configure Production CORS
**Duration**: 15 minutes  
**Dependencies**: Task 9.1, 9.2  
**Type**: Configuration  

**What to do**:
- Update CORS settings to allow frontend domain only
- Test API calls from production frontend

**Acceptance Criteria**:
- No CORS errors in production

---

#### Task 9.4: Setup Monitoring
**Duration**: 30 minutes  
**Dependencies**: Task 9.2  
**Type**: Monitoring  

**What to do**:
- Configure Sentry for error tracking
- Test error reporting

**Acceptance Criteria**:
- Errors logged to Sentry

---

### Phase 10: Reusable Intelligence

#### Task 10.1: Create RAG Implementation Skill
**Duration**: 45 minutes  
**Dependencies**: Task 3.4  
**Type**: Intelligence Creation  

**What to do**:
- Design skill for "Implement RAG Pipeline"
- Document in `history/prompts/001-rag-skill.md`

**Acceptance Criteria**:
- Skill tested with Gemini CLI
- PHR documents what worked

---

#### Task 10.2: Create Auth Integration Skill
**Duration**: 45 minutes  
**Dependencies**: Task 5.5  
**Type**: Intelligence Creation  

**What to do**:
- Design skill for "Implement Better-auth Integration"
- Document in `history/prompts/002-auth-skill.md`

**Acceptance Criteria**:
- Skill tested with Gemini CLI
- PHR documents prompts that succeeded

---

#### Task 10.3: Create Content Writer Subagent
**Duration**: 60 minutes  
**Dependencies**: Task 2.7  
**Type**: Intelligence Creation  

**What to do**:
- Design subagent specialized in technical writing
- Persona: Technical educator, pedagogical expert
- Test with sample chapter

**Acceptance Criteria**:
- Subagent generates quality chapter content
- PHR documents usage patterns

---

## Task Summary

**Total Tasks**: 53  
**Estimated Time**: 90-120 hours  

**Critical Path** (Base Deliverable):
- Phase 1: 4 tasks (2 hours)
- Phase 2: 7 tasks (12 hours)
- Phase 3: 5 tasks (5 hours)
- Phase 4: 4 tasks (4 hours)
- Phase 8: 4 tasks (5 hours)
- Phase 9: 4 tasks (2 hours)
**Total**: 30 hours minimum for base deliverable

**Bonus Features**:
- Phase 5: 5 tasks (5 hours)
- Phase 6: 4 tasks (4 hours)
- Phase 7: 4 tasks (4 hours)
- Phase 10: 3 tasks (4 hours)
**Total**: +17 hours for all bonuses

---

## Task Execution Order

**Week 1** (30-40 hours):
- Days 1-2: Phase 1 (Foundation)
- Days 3-5: Phase 2 (Content Creation)
- Days 6-7: Phase 3 (RAG Backend)

**Week 2** (30-40 hours):
- Days 1-2: Phase 4 (Chatbot Frontend)
- Days 3-4: Phase 5 (Authentication)
- Day 5: Phase 6 (Personalization)
- Day 6: Phase 7 (Translation)
- Day 7: Phase 8 (Testing)

**Week 3** (20-30 hours):
- Day 1: Phase 8 (Testing continued)
- Day 2: Phase 9 (Deployment)
- Day 3: Phase 10 (Reusable Intelligence)
- Days 4-5: Buffer for debugging, documentation
- Days 6-7: Final testing, demo video, submission

---

## Checkpoint Strategy

**Checkpoint 1** (After Phase 1):
- Validate: All external APIs connected
- Decision: Proceed to content creation

**Checkpoint 2** (After Phase 2):
- Validate: All chapters written and reviewed
- Decision: Proceed to RAG backend

**Checkpoint 3** (After Phase 4):
- Validate: Chatbot functional end-to-end
- Decision: Base deliverable complete, start bonuses or submit

**Checkpoint 4** (After Phase 7):
- Validate: All bonus features working
- Decision: Proceed to testing and deployment

**Checkpoint 5** (After Phase 9):
- Validate: Production deployment successful
- Decision: Create demo video, submit project



# Implementation Tasks: Physical AI Book with Multi-Agent RAG Chatbot

**Version**: 2.1.0  
**Status**: Active  
**Created**: 2024-12-06  
**Updated**: 2024-12-06 (OpenAI Agents SDK with Gemini API Client)  
**Dependencies**: plan.md (v2.0)

---

## IMPORTANT: OpenAI Agents SDK with Gemini API

**Architecture Pattern**: We use OpenAI Agents SDK with Gemini as the LLM backend by configuring a custom AsyncOpenAI client with Gemini's base_url and API key. This allows us to leverage the Agents SDK's framework while using Gemini Flash 2.5 for cost-effectiveness.

```python
# Pattern from Gemini OpenAI Compatibility docs
from openai import AsyncOpenAI
from agents import Agent, set_default_openai_client

# Configure Gemini as OpenAI-compatible client
gemini_client = AsyncOpenAI(
    api_key=os.getenv("GEMINI_API_KEY"),
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

# Set globally for all agents
set_default_openai_client(gemini_client, use_for_tracing=False)

# Now create agents that use Gemini under the hood
agent = Agent(
    name="Concept Explainer",
    model="gemini-2.5-flash",  # Gemini model
    instructions="You are a patient educator..."
)
```

---

## Task Breakdown Structure

### Phase 1: Foundation Setup âœ… COMPLETED

[Previous tasks 1.1-1.4 remain unchanged]

---

### Phase 2: Book Content Creation âœ… COMPLETED

[Previous tasks 2.1-2.7 remain unchanged]

---

### Phase 3: RAG Backend âœ… COMPLETED

[Previous tasks 3.1-3.5 completed and now being enhanced]

---

### Phase 3.5: Multi-Agent System Implementation (CURRENT PHASE)

#### Task 3.5.1: Setup OpenAI Agents SDK with Gemini Client
**Duration**: 45 minutes  
**Dependencies**: Task 3.5 (RAG Backend Complete)  
**Type**: Setup  
**Priority**: P0

**What to do**:
1. Install OpenAI Agents SDK:
   ```bash
   pip install agents openai
   ```

2. Create `backend/app/config/` directory

3. Create `backend/app/config/settings.py`:
   ```python
   from pydantic_settings import BaseSettings
   from functools import lru_cache
   
   class Settings(BaseSettings):
       # API Keys
       GEMINI_API_KEY: str  # Primary LLM for agents
       QDRANT_URL: str
       QDRANT_API_KEY: str
       NEON_DATABASE_URL: str
       
       # Gemini OpenAI-compatible endpoint
       GEMINI_BASE_URL: str = "https://generativelanguage.googleapis.com/v1beta/openai/"
       GEMINI_MODEL: str = "gemini-2.5-flash"
       
       # JWT Settings
       JWT_SECRET_KEY: str
       JWT_ALGORITHM: str = "HS256"
       ACCESS_TOKEN_EXPIRE_MINUTES: int = 15
       REFRESH_TOKEN_EXPIRE_DAYS: int = 7
       
       # RAG Settings
       QDRANT_COLLECTION_NAME: str = "book_chapters_en"
       QDRANT_VECTOR_SIZE: int = 3072  # gemini-embedding-001
       RAG_TOP_K: int = 5
       RAG_SCORE_THRESHOLD: float = 0.6
       RAG_MAX_CONTEXT_TOKENS: int = 6000
       
       # Agent Settings
       AGENT_TEMPERATURE: float = 0.3
       AGENT_MAX_TOKENS: int = 800
       
       # CORS Settings
       CORS_ORIGINS: list = ["http://localhost:3000"]
       
       class Config:
           env_file = ".env"
           case_sensitive = True
   
   @lru_cache()
   def get_settings():
       return Settings()
   ```

4. Create `backend/app/config/agents_config.py` with 3 agent definitions (detailed below)

5. Create `backend/app/config/llm_client.py`:
   ```python
   from openai import AsyncOpenAI
   from agents import set_default_openai_client
   from .settings import get_settings
   
   def setup_gemini_client():
       """Configure OpenAI Agents SDK to use Gemini as backend LLM."""
       settings = get_settings()
       
       # Create Gemini-backed AsyncOpenAI client
       gemini_client = AsyncOpenAI(
           api_key=settings.GEMINI_API_KEY,
           base_url=settings.GEMINI_BASE_URL
       )
       
       # Set as default client for all agents
       # use_for_tracing=False to avoid OpenAI tracing errors
       set_default_openai_client(gemini_client, use_for_tracing=False)
       
       return gemini_client
   ```

**Acceptance Criteria**:
- OpenAI Agents SDK installed
- `config/settings.py` loads environment variables
- `config/llm_client.py` configures Gemini as OpenAI-compatible client
- `config/agents_config.py` has 3 agent definitions
- No import errors
- Test: Can create Agent instance using Gemini model

**Validation**:
```bash
# Test Gemini client setup
python -c "
from app.config.llm_client import setup_gemini_client
from agents import Agent

setup_gemini_client()
agent = Agent(name='Test', model='gemini-2.5-flash', instructions='Test agent')
print('âœ“ Gemini client configured successfully')
"
```

**Gemini CLI Prompt**:
```
I need to configure OpenAI Agents SDK to use Gemini API as the LLM backend.

Create these files in backend/app/config/:

1. settings.py:
   - Use Pydantic BaseSettings
   - GEMINI_API_KEY (primary LLM)
   - GEMINI_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/openai/"
   - GEMINI_MODEL = "gemini-2.5-flash"
   - QDRANT settings
   - NEON_DATABASE_URL
   - JWT settings
   - RAG settings (top_k=5, threshold=0.6)

2. llm_client.py:
   - Import AsyncOpenAI, set_default_openai_client, settings
   - Function: setup_gemini_client()
   - Create AsyncOpenAI with gemini_api_key and base_url
   - Call set_default_openai_client(client, use_for_tracing=False)
   - Return client

3. agents_config.py:
   - Define 3 agents (concept_explainer, code_helper, troubleshoot)
   - Each has: name, description, system_prompt, task_keywords, rag_filter_keywords
   - Detailed system prompts for each persona
   - Routing config with default_agent and min_confidence

Follow Gemini OpenAI compatibility docs pattern.
Use type hints throughout.
```

---

#### Task 3.5.2: Implement Agent Orchestrator
**Duration**: 60 minutes  
**Dependencies**: Task 3.5.1  
**Type**: Feature Implementation  
**Priority**: P0

**What to do**:
1. Create `backend/app/agents/` directory
2. Create `backend/app/agents/orchestrator.py`
3. Implement `AgentOrchestrator` class:
   - `classify_query(query: str) -> tuple[str, float]`
   - Keyword-based classification using agents_config
   - Calculate confidence score
   - Return (agent_name, confidence)
   - Default to "concept_explainer" if confidence < 0.6
4. Add structured logging for routing decisions
5. Write unit tests

**Implementation Pattern**:
```python
from app.config.agents_config import AGENTS_CONFIG, ROUTING_CONFIG
import logging

logger = logging.getLogger(__name__)

class AgentOrchestrator:
    def __init__(self):
        self.agents_config = AGENTS_CONFIG
        self.routing_config = ROUTING_CONFIG
    
    def classify_query(self, query: str) -> tuple[str, float]:
        """
        Classify query to determine appropriate agent.
        
        Returns:
            (agent_name, confidence_score)
        """
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        scores = {}
        for agent_name, config in self.agents_config.items():
            keywords = config["task_keywords"]
            matches = sum(1 for kw in keywords if kw in query_lower)
            confidence = matches / len(keywords) if keywords else 0
            scores[agent_name] = confidence
        
        # Get agent with highest score
        best_agent = max(scores, key=scores.get)
        best_score = scores[best_agent]
        
        # Use default if confidence too low
        min_confidence = self.routing_config["min_confidence"]
        if best_score < min_confidence:
            logger.info(f"Low confidence ({best_score:.2f}), using default agent")
            return self.routing_config["default_agent"], best_score
        
        logger.info(f"Routed to {best_agent} with confidence {best_score:.2f}")
        return best_agent, best_score
    
    def route_query(self, query: str) -> str:
        """Simple wrapper that returns agent name only."""
        agent_name, _ = self.classify_query(query)
        return agent_name
```

**Acceptance Criteria**:
- Orchestrator classifies queries correctly (test with 10+ examples)
- Handles edge cases (empty query, ambiguous query)
- Confidence scoring works
- Logging shows routing decisions
- Test coverage > 80%

**Test Cases**:
```python
# In tests/unit/test_orchestrator.py
def test_concept_query():
    orchestrator = AgentOrchestrator()
    agent, confidence = orchestrator.classify_query("What is ROS 2?")
    assert agent == "concept_explainer"
    assert confidence > 0.7

def test_code_query():
    orchestrator = AgentOrchestrator()
    agent, confidence = orchestrator.classify_query("Show me code for a publisher")
    assert agent == "code_helper"
    assert confidence > 0.75

def test_troubleshoot_query():
    orchestrator = AgentOrchestrator()
    agent, confidence = orchestrator.classify_query("Error: node not found")
    assert agent == "troubleshoot"
    assert confidence > 0.8
```

---

#### Task 3.5.3: Implement Concept Explainer Agent
**Duration**: 45 minutes  
**Dependencies**: Task 3.5.2  
**Type**: Agent Implementation  
**Priority**: P0

**What to do**:
1. Create `backend/app/agents/concept_agent.py`
2. Implement `ConceptExplainerAgent` class
3. Uses OpenAI Agents SDK with Gemini backend (already configured)
4. Method: `process_query(query, rag_context, sources) -> dict`

**Implementation Pattern**:
```python
from agents import Agent
from app.config.agents_config import AGENTS_CONFIG
from app.config.settings import get_settings
import logging

logger = logging.getLogger(__name__)

class ConceptExplainerAgent:
    def __init__(self):
        self.config = AGENTS_CONFIG["concept_explainer"]
        self.settings = get_settings()
        
        # Create agent using OpenAI Agents SDK (with Gemini backend)
        self.agent = Agent(
            name=self.config["name"],
            model=self.settings.GEMINI_MODEL,  # "gemini-2.5-flash"
            instructions=self.config["system_prompt"]
        )
    
    async def process_query(
        self,
        query: str,
        rag_context: str,
        sources: list[dict]
    ) -> dict:
        """
        Process query using Concept Explainer persona.
        
        Args:
            query: User's question
            rag_context: Retrieved chapter excerpts from RAG
            sources: List of source metadata
        
        Returns:
            dict with answer, sources, agent info
        """
        try:
            # Build user message with RAG context
            user_message = f"""Context from book chapters:

{rag_context}

---

Question: {query}

Please explain this concept clearly using analogies and the provided context. Always cite the source chapters."""

            # Run agent (uses Gemini via configured client)
            from agents import Runner
            result = await Runner.run_async(
                self.agent,
                user_message
            )
            
            answer = result.final_output
            
            logger.info(f"Concept Agent generated response: {len(answer)} chars")
            
            return {
                "answer": answer,
                "sources": sources,
                "agent": self.config["name"],
                "agent_type": "concept_explainer"
            }
            
        except Exception as e:
            logger.error(f"Concept Agent error: {e}")
            raise
```

**Acceptance Criteria**:
- Agent generates educational explanations
- Uses RAG context in responses
- Includes source citations
- Tone matches "patient educator" persona
- Handles Gemini API errors gracefully

---

#### Task 3.5.4: Implement Code Helper Agent
**Duration**: 45 minutes  
**Dependencies**: Task 3.5.3  
**Type**: Agent Implementation  
**Priority**: P0

**What to do**:
- Create `backend/app/agents/code_agent.py`
- Similar structure to Concept Agent
- Use "code_helper" config
- Focus on code examples and best practices

**Key Difference**:
- System prompt emphasizes providing working code
- Post-process to ensure code blocks formatted correctly

**Gemini CLI Prompt**:
```
Implement CodeHelperAgent in app/agents/code_agent.py

Pattern: Same as ConceptExplainerAgent but:
1. Load "code_helper" config from agents_config
2. Create Agent with gemini-2.5-flash model
3. System prompt focuses on code examples
4. process_query method same structure
5. Ensure response includes ```python code blocks

Use OpenAI Agents SDK with Gemini backend (already configured in llm_client.py).
Include error handling and logging.
```

---

#### Task 3.5.5: Implement Troubleshooting Agent
**Duration**: 45 minutes  
**Dependencies**: Task 3.5.4  
**Type**: Agent Implementation  
**Priority**: P0

**What to do**:
- Create `backend/app/agents/troubleshoot_agent.py`
- Use "troubleshoot" config
- Format responses as step-by-step solutions

**Gemini CLI Prompt**:
```
Implement TroubleshootingAgent in app/agents/troubleshoot_agent.py

Pattern: Same as ConceptExplainerAgent but:
1. Load "troubleshoot" config
2. Create Agent with diagnostic persona
3. Format responses as numbered steps
4. Include clarifying questions if needed

Follow same async pattern with OpenAI Agents SDK + Gemini.
```


---

#### Task 3.5.6: Refactor RAG Service for Agent Integration
**Duration**: 60 minutes  
**Dependencies**: Task 3.5.5, Task 3.5 (Phase 3 complete)  
**Type**: Refactoring  
**Priority**: P0

**What to do**:
1. Reorganize Phase 3 code into `backend/app/chatbot/` directory:
   - Move `embedder.py` â†’ `app/chatbot/embedder.py`
   - Move `vector_search.py` â†’ `app/chatbot/vector_search.py`
   - Move `context_builder.py` â†’ `app/chatbot/context_builder.py`
2. Create `app/chatbot/rag_service.py`:
   - Main entry point for RAG operations
   - Method: `retrieve_context(query: str, agent_type: str) -> dict`
     - Generate query embedding (Gemini)
     - Apply agent-specific filters (based on agent's rag_filter_keywords)
     - Search Qdrant with filters
     - Build context using context_builder
     - Return: {context: str, sources: List[dict], relevance: float}
3. Add agent-specific filtering logic:
   - If agent_type == "code_helper": prioritize code examples
   - If agent_type == "troubleshoot": prioritize troubleshooting sections
4. Test RAG service independently

**Acceptance Criteria**:
- RAG service callable by agents: `rag_service.retrieve_context(query, agent_type)`
- Returns context string + source metadata + average relevance score
- Agent-specific filtering works (code agent gets more code examples)
- No breaking changes to existing Phase 3 functionality
- All imports updated correctly

**Directory Structure After Refactoring**:
```
app/chatbot/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ rag_service.py          # Main RAG orchestration
â”œâ”€â”€ embedder.py             # Gemini embeddings
â”œâ”€â”€ vector_search.py        # Qdrant queries
â””â”€â”€ context_builder.py      # Context assembly
```

**Gemini CLI Prompt**:
```
Refactor Phase 3 RAG code into app/chatbot/ module with agent integration:

1. Move existing code:
   - embedder.py, vector_search.py, context_builder.py â†’ app/chatbot/

2. Create app/chatbot/rag_service.py:
   - Class: RAGService
   - Method: retrieve_context(query: str, agent_type: str) -> dict
   - Steps:
     a. Generate embedding (use embedder)
     b. Apply agent-specific Qdrant filters (metadata filtering)
     c. Search vector DB (use vector_search)
     d. Build context (use context_builder)
     e. Return dict with context, sources, relevance

3. Add agent-specific filtering:
   - code_helper: filter payload.content contains "```" or "code"
   - troubleshoot: filter payload.section_title contains "troubleshoot" or "error"

Use async/await throughout.
Include comprehensive error handling.
```

---

#### Task 3.5.7: Update Chat API Endpoint for Agent Orchestration
**Duration**: 45 minutes  
**Dependencies**: Task 3.5.6  
**Type**: API Update  
**Priority**: P0

**What to do**:
1. Create/update `backend/app/routes/` directory
2. Create `backend/app/routes/chat.py`
3. Implement POST /api/v1/chat endpoint:
   - Parse request (ChatRequest schema)
   - Call orchestrator to route query
   - Call RAG service to get context
   - Call appropriate agent with query + context
   - Format response (ChatResponse schema)
   - Log performance metrics
   - Save to chat_history table (if user authenticated)
4. Update response schema to include agent metadata
5. Add error handling for each step
6. Test endpoint end-to-end

**Request/Response Flow**:
```
Request: POST /api/v1/chat
{
  "message": "What is ROS 2?",
  "context": null,  # Optional selected text
  "conversation_history": []  # Optional
}

Flow:
1. Orchestrator.route_query("What is ROS 2?") â†’ "concept_explainer"
2. RAGService.retrieve_context("What is ROS 2?", "concept_explainer") â†’ {context, sources}
3. ConceptExplainerAgent.process_query("What is ROS 2?", context, sources) â†’ {answer, sources, agent}
4. Return response

Response: 200 OK
{
  "answer": "ROS 2 (Robot Operating System 2) is...",
  "sources": [
    {
      "chapter_id": "module-1-ros2/01-introduction",
      "chapter_title": "Introduction to ROS 2",
      "url": "/docs/module-1-ros2/01-introduction",
      "relevance_score": 0.87
    }
  ],
  "agent_used": "concept_explainer",
  "agent_name": "Concept Explainer Agent",
  "agent_confidence": 0.85,
  "context_relevance": 0.87,
  "metadata": {
    "response_time_ms": 2341,
    "rag_time_ms": 456,
    "agent_time_ms": 1785
  }
}
```

**Acceptance Criteria**:
- Endpoint accessible at POST /api/v1/chat
- Request validation works (Pydantic)
- Orchestrator routes query to appropriate agent
- RAG retrieves context before agent processes
- Response includes all required fields
- Performance metrics logged
- Response time P95 < 3 seconds
- Error handling for orchestrator, RAG, and agent failures

**Gemini CLI Prompt**:
```
Implement POST /api/v1/chat endpoint in app/routes/chat.py for agent-based chatbot:

Flow:
1. Validate request (ChatRequest schema)
2. Route query using AgentOrchestrator
3. Retrieve RAG context using RAGService
4. Call appropriate agent (concept_explainer, code_helper, or troubleshoot)
5. Format response (ChatResponse schema with agent metadata)
6. Log performance metrics
7. Return response

Include:
- Async/await throughout
- Error handling for each step (orchestrator failure, RAG failure, agent failure)
- Performance timing (track rag_time_ms, agent_time_ms)
- Structured logging

Response schema:
- answer (str)
- sources (list of dicts)
- agent_used (str)
- agent_name (str)
- agent_confidence (float)
- context_relevance (float)
- metadata (dict with timing)
```

---

#### Task 3.5.8: Update Chat History Model with Agent Metadata
**Duration**: 20 minutes  
**Dependencies**: Task 3.5.7  
**Type**: Database Update  
**Priority**: P1

**What to do**:
1. Update `app/db/models/chat_history.py`:
   - Add columns: agent_used, agent_confidence, context_relevance
   - Add columns: response_time_ms, rag_time_ms, agent_time_ms
2. Create Alembic migration
3. Run migration on local database
4. Update chat_history saving logic in chat.py endpoint

**Acceptance Criteria**:
- chat_history table has new columns
- Migration runs successfully
- Chat endpoint saves agent metadata to database
- Old chat history still accessible

**Alembic Migration**:
```bash
alembic revision --autogenerate -m "Add agent metadata to chat_history"
alembic upgrade head
```

---

#### Task 3.5.9: Write Tests for Agent System
**Duration**: 60 minutes  
**Dependencies**: Task 3.5.8  
**Type**: Testing  
**Priority**: P1

**What to do**:
1. Create `tests/unit/test_orchestrator.py`:
   - Test query classification with various inputs
   - Test confidence scoring
   - Test default agent fallback
2. Create `tests/unit/test_agents.py`:
   - Mock GEMINI API responses
   - Test each agent's process_query method
   - Verify persona differences in responses
3. Create `tests/integration/test_chat_api.py`:
   - Test end-to-end: request â†’ orchestrator â†’ RAG â†’ agent â†’ response
   - Test error scenarios
   - Test performance (response time < 3s)
4. Run tests: `pytest tests/ --cov=app/agents --cov=app/chatbot`

**Acceptance Criteria**:
- All tests pass
- Coverage > 75% for agents/ and chatbot/ modules
- Integration test verifies full flow
- Performance test confirms <3s response time

---

### Phase 3.5 Summary

**Total Tasks**: 9  
**Estimated Time**: 6-8 hours  
**Priority**: P0 (Required for base deliverable)

**Completion Checklist**:
- [ ] Task 3.5.1: OpenAI SDK + config setup
- [ ] Task 3.5.2: Orchestrator implemented
- [ ] Task 3.5.3: Concept Agent implemented
- [ ] Task 3.5.4: Code Agent implemented
- [ ] Task 3.5.5: Troubleshoot Agent implemented
- [ ] Task 3.5.6: RAG service refactored for agents
- [ ] Task 3.5.7: Chat API updated with agent flow
- [ ] Task 3.5.8: Database updated with agent metadata
- [ ] Task 3.5.9: Tests written and passing

---

### Phase 4: Chatbot Frontend (Minor Updates)

#### Task 4.1-4.4: Previous Tasks âœ… COMPLETED

**Minor Update Required**: Task 4.5 (Add Agent Display)

#### Task 4.5: Display Agent Information in Chat UI
**Duration**: 30 minutes  
**Dependencies**: Task 3.5.7, Task 4.1-4.4  
**Type**: UI Enhancement  
**Priority**: P1

**What to do**:
1. Update ChatMessage component to display agent name
2. Add agent icon/avatar (optional):
   - Concept: ðŸŽ“ or lightbulb icon
   - Code: ðŸ’» or code icon
   - Troubleshoot: ðŸ”§ or wrench icon
3. Show agent transition messages:
   - "Concept Explainer Agent is thinking..."
   - "Connecting you to Code Helper Agent..."
4. Style agent badge in response

**Acceptance Criteria**:
- Agent name visible in each response
- Visual distinction between different agents (color/icon)
- Loading state shows which agent is processing
- Mobile responsive

---

### Phases 5-10: No Changes

Phases 5 (Authentication), 6 (Personalization), 7 (Translation), 8 (Testing), 9 (Deployment), 10 (Reusable Intelligence) remain unchanged from original tasks.md.

---

## Updated Task Execution Order

**Week 1** (30-40 hours):
- Days 1-2: Phase 1 (Foundation) âœ…
- Days 3-5: Phase 2 (Content Creation) âœ…
- Days 6-7: Phase 3 (RAG Backend) âœ…

**Week 2** (30-40 hours):
- Days 1-2: **Phase 3.5 (Multi-Agent System)** â† CURRENT
- Days 3-4: Phase 4 (Chatbot Frontend + Agent Display)
- Days 5-6: Phase 5 (Authentication)
- Day 7: Phase 6 (Personalization)

**Week 3** (20-30 hours):
- Day 1: Phase 7 (Translation)
- Day 2: Phase 8 (Testing)
- Day 3: Phase 9 (Deployment)
- Days 4-5: Phase 10 (Reusable Intelligence)
- Days 6-7: Final testing, demo video, submission

---

## Checkpoint Strategy (Updated)

**Checkpoint 1** (After Phase 1): âœ… PASSED
- All external APIs connected

**Checkpoint 2** (After Phase 2): âœ… PASSED
- All chapters written and reviewed

**Checkpoint 3** (After Phase 3): âœ… PASSED
- Basic RAG pipeline functional

**Checkpoint 3.5** (After Phase 3.5): â† NEXT CHECKPOINT
- Validate:
  - [ ] 3 agents implemented and tested
  - [ ] Orchestrator routes queries correctly (>80% accuracy)
  - [ ] Agents use RAG context before responding
  - [ ] API response includes agent metadata
  - [ ] Response time P95 < 3 seconds
- Decision: Proceed to Phase 4 (Frontend Updates)

**Checkpoint 4** (After Phase 4): 
- Validate: Chatbot functional with agent display
- Decision: Base deliverable complete, start bonuses or deploy

**Checkpoint 5** (After Phase 9):
- Validate: Production deployment successful
- Decision: Create demo video, submit project

---

## Revision History

| Version | Date       | Changes                                      | Author |
|---------|------------|----------------------------------------------|--------|
| 1.0.0   | 2024-12-06 | Initial tasks                                | AI     |
| 2.0.0   | 2024-12-06 | Added Phase 3.5 multi-agent tasks            | AI     |